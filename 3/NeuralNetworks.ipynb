{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this exercise we will train some neural network using Pytorch.\\nInstallation instructions: https://pytorch.org/get-started/locally/\\n\\nI recommend to run this exercise in colab using GPU or in kaggle notebooks.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this exercise we will train some neural network using Pytorch.\n",
    "Installation instructions: https://pytorch.org/get-started/locally/\n",
    "\n",
    "I recommend to run this exercise in colab using GPU or in kaggle notebooks.\n",
    "You should find something like: Runtime -> Change runtime type -> T4 GPU to access GPU there.\n",
    "Everything thing you need is preinstalled there.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because outside world is ugly\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch is library for dealing with neural networks (and automatic gradients)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Torchvision is helper library for pytorch to deal with computer vision\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usamec/anaconda3/envs/magic_train/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630815121/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepare dataset, whole lecture will be done over MNIST dataset\"\"\"\n",
    "batch_size_train = 256\n",
    "batch_size_test = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256]) tensor([1, 4, 0, 8, 7, 7, 2, 9, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# What is in train?\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape, y[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e400e0730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X: 256 images, 1 channel (black/white), 28x28 image\n",
    "# We will flatten each image into one vector for now\n",
    "# Y: One number (category of image)\n",
    "# Images looks like this:\n",
    "\n",
    "plt.imshow(x[0,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight', torch.Size([10, 784])), ('bias', torch.Size([10]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple linear model from last lecture\n",
    "# This just computes scores for each class\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "[(name, p.shape) for name, p in model_linear.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4573, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compute loss for one batch of images\n",
    "output = model_linear(x.flatten(1))\n",
    "log_probs = F.log_softmax(output, dim=-1)    # log_softmax(.) = log(softmax(.))\n",
    "loss = F.nll_loss(log_probs, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4573, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch has many loss functions, which look similar but take different things,\n",
    "# so take care\n",
    "# Here cross entropy(output, y) is same as nll_loss(log_softmax(output), y)\n",
    "loss2 = F.cross_entropy(output, y)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0013,  0.0013,  0.0013,  ...,  0.0013,  0.0013,  0.0013],\n",
       "        [ 0.0209,  0.0209,  0.0209,  ...,  0.0209,  0.0209,  0.0209],\n",
       "        [ 0.0345,  0.0345,  0.0345,  ...,  0.0345,  0.0345,  0.0345],\n",
       "        ...,\n",
       "        [-0.0437, -0.0437, -0.0437,  ..., -0.0437, -0.0437, -0.0437],\n",
       "        [ 0.0108,  0.0108,  0.0108,  ...,  0.0108,  0.0108,  0.0108],\n",
       "        [-0.0161, -0.0161, -0.0161,  ..., -0.0161, -0.0161, -0.0161]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to train model, we need to get gradients\n",
    "# This is easy, gradient will magically appear\n",
    "loss.backward()\n",
    "model_linear.weight.grad\n",
    "\n",
    "# If you really want to do things by hand\n",
    "# model_linear.weight.data = model_linear.weight.data - 0.01 * model_linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0026,  0.0026,  0.0026,  ...,  0.0026,  0.0026,  0.0026],\n",
       "        [ 0.0418,  0.0418,  0.0418,  ...,  0.0418,  0.0418,  0.0418],\n",
       "        [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690],\n",
       "        ...,\n",
       "        [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
       "        [ 0.0217,  0.0217,  0.0217,  ...,  0.0217,  0.0217,  0.0217],\n",
       "        [-0.0322, -0.0322, -0.0322,  ..., -0.0322, -0.0322, -0.0322]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we calculate gradient second time, it will accumulate\n",
    "output = model_linear(x.flatten(1))\n",
    "log_probs = F.log_softmax(output, dim=-1)    # log_softmax(.) = log(softmax(.))\n",
    "loss = F.nll_loss(log_probs, y)\n",
    "loss.backward()\n",
    "model_linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 2.007697594799894\n",
      "epoch 1 training loss 1.245814481187374\n",
      "epoch 2 training loss 1.1943411404782154\n",
      "epoch 3 training loss 1.0507514679685552\n",
      "epoch 4 training loss 1.121161828015713\n"
     ]
    }
   ],
   "source": [
    "# Lets optimize, this is typical pytorch training loop you will see a lot with some modifications\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "optimizer = torch.optim.SGD(model_linear.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    for x, y in train_loader:\n",
    "        # Here we calculate output from the model, note that we flatten the input (converts 256,1,28,28 to 256,784) \n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        # Here we calculate loss\n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        # Here we calculate gradients, first we need to zero previous ones\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        # And here we update the weights\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "    print(\"epoch\", epoch, \"training loss\", total_loss / total_loss_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 2.043 training accuracy 0.834 testing accuracy 0.886\n",
      "epoch 1 training loss 1.231 training accuracy 0.870 testing accuracy 0.720\n",
      "epoch 2 training loss 1.239 training accuracy 0.875 testing accuracy 0.576\n",
      "epoch 3 training loss 1.169 training accuracy 0.878 testing accuracy 0.904\n",
      "epoch 4 training loss 1.019 training accuracy 0.885 testing accuracy 0.818\n",
      "epoch 5 training loss 1.104 training accuracy 0.883 testing accuracy 0.889\n",
      "epoch 6 training loss 1.073 training accuracy 0.882 testing accuracy 0.744\n",
      "epoch 7 training loss 1.062 training accuracy 0.885 testing accuracy 0.879\n",
      "epoch 8 training loss 1.033 training accuracy 0.887 testing accuracy 0.844\n",
      "epoch 9 training loss 1.081 training accuracy 0.885 testing accuracy 0.841\n"
     ]
    }
   ],
   "source": [
    "# Optimize and compute training accuracy and test accuracy\n",
    "\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "optimizer = torch.optim.SGD(model_linear.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        # This calulcates accuracy\n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 1.582 training accuracy 0.535 testing accuracy 0.647\n",
      "epoch 1 training loss 0.743 training accuracy 0.783 testing accuracy 0.888\n",
      "epoch 2 training loss 0.423 training accuracy 0.888 testing accuracy 0.876\n",
      "epoch 3 training loss 0.305 training accuracy 0.917 testing accuracy 0.868\n",
      "epoch 4 training loss 0.273 training accuracy 0.924 testing accuracy 0.919\n",
      "epoch 5 training loss 0.286 training accuracy 0.916 testing accuracy 0.855\n",
      "epoch 6 training loss 0.261 training accuracy 0.927 testing accuracy 0.933\n",
      "epoch 7 training loss 0.210 training accuracy 0.940 testing accuracy 0.922\n",
      "epoch 8 training loss 0.193 training accuracy 0.944 testing accuracy 0.925\n",
      "epoch 9 training loss 0.173 training accuracy 0.949 testing accuracy 0.939\n"
     ]
    }
   ],
   "source": [
    "# Let's train neural network, we only change model and maybe learning rate\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0.0)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = model(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        output = model(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# How to run on cuda (this optional, if you have GPU access, vyuka does not have one, colab does)\\n# No need to run this cell\\n\\nmodel = nn.Sequential(\\n    nn.Linear(28*28, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 10)\\n)\\n# Move model to GPU. Instead of x.cuda() you can also use x.to(device) where device is \"cpu\" or \"cuda\" so your code\\n# is nicely parametrized.\\n\\nmodel.cuda()\\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\\n\\nfor epoch in range(20):\\n    total_loss = 0\\n    total_loss_cc = 0\\n    total_good = 0\\n    total_samples = 0\\n    \\n    for x, y in train_loader:\\n        # Move input and labels to GPU\\n        output = model(x.cuda().flatten(1))\\n        y = y.cuda()\\n        log_probs = F.log_softmax(output, dim=-1)\\n        \\n        prediction = log_probs.argmax(dim=-1)\\n        total_good += (prediction == y).sum().item()\\n        total_samples += y.shape[0]\\n        \\n        batch_loss = F.nll_loss(log_probs, y)\\n        optimizer.zero_grad()\\n        batch_loss.backward()\\n        optimizer.step()\\n        total_loss += batch_loss.item()\\n        total_loss_cc += 1\\n        \\n    total_good_test = 0\\n    total_samples_test = 0\\n    for x, y in test_loader:\\n        # Move input and labels to GPU\\n        y = y.cuda()\\n        output = model(x.cuda().flatten(1))\\n        log_probs = F.log_softmax(output, dim=-1)\\n        \\n        prediction = log_probs.argmax(dim=-1)\\n        total_good_test += (prediction == y).sum().item()\\n        total_samples_test += y.shape[0]\\n        \\n    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \\n          \"training accuracy %.3f\" % (total_good / total_samples), \\n          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))\\n          \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# How to run on cuda (this optional, if you have GPU access, vyuka does not have one, colab does)\n",
    "# No need to run this cell\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "# Move model to GPU. Instead of x.cuda() you can also use x.to(device) where device is \"cpu\" or \"cuda\" so your code\n",
    "# is nicely parametrized.\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        # Move input and labels to GPU\n",
    "        output = model(x.cuda().flatten(1))\n",
    "        y = y.cuda()\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        # Move input and labels to GPU\n",
    "        y = y.cuda()\n",
    "        output = model(x.cuda().flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))\n",
    "          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 1.\\nIn example above figure out best settings for learning rate a momentum in the optimizer.\\nKeep the number of epochs to 10. Also keep the learning constant during training.\\nSumarize your findings in some chart or table.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 1.\n",
    "In example above figure out best settings for learning rate a momentum in the optimizer.\n",
    "Keep the number of epochs to 10. Also keep the learning constant during training.\n",
    "Sumarize your findings in some chart or table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 2.\\nHere is network with 2 hidden layers (each additional hidden layer we add nn.Linear(256,256) and nn.ReLU())\\n\\nmodel = nn.Sequential(\\n    nn.Linear(28*28, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 10)\\n)\\n\\nHow does adding more hidden layers influence final accuracy (try 3-6 hidden layers).\\nAlways figure out the best optimizer settings. Again run for 10 epochs.\\nAgain produce some chart or table.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 2.\n",
    "Here is network with 2 hidden layers (each additional hidden layer we add nn.Linear(256,256) and nn.ReLU())\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "How does adding more hidden layers influence final accuracy (try 3-6 hidden layers).\n",
    "Always figure out the best optimizer settings. Again run for 10 epochs.\n",
    "Again produce some chart or table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTask 3.\\nHere is an example of convolutional network. Do not change it for this task.\\nOnly figure out best settings for the optimizer. Also run for only 10 epochs.\\nmodel = nn.Sequential(\\n    nn.Conv2d(1, 16, 5, padding=1, stride=2),  # Here we get 14x14 image with 16 channels\\n    nn.ReLU(),\\n    nn.Conv2d(16, 16, 3, padding='same'),\\n    nn.ReLU(),\\n    nn.Conv2d(16, 32, 3, padding=1, stride=2),  # Here we get 7x7 image with 32 channels\\n    nn.ReLU(),\\n    nn.Conv2d(32, 32, 3, padding='same'),\\n    nn.ReLU(),\\n    nn.Flatten(),\\n    nn.Linear(7*7*32, 10)\\n)\\n\\nRun training of convolutional network and report the results and compare with best results from above.\\nAlso, there is no need to flatten the input now. \\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 3.\n",
    "Here is an example of convolutional network. Do not change it for this task.\n",
    "Only figure out best settings for the optimizer. Also run for only 10 epochs.\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 5, padding=1, stride=2),  # Here we get 14x14 image with 16 channels\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, padding=1, stride=2),  # Here we get 7x7 image with 32 channels\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(7*7*32, 10)\n",
    ")\n",
    "\n",
    "Run training of convolutional network and report the results and compare with best results from above.\n",
    "Also, there is no need to flatten the input now. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 4 bonus:\\nChange neural architecture like you want and achive best accuracy in 10 epochs.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 4 bonus:\n",
    "Change neural architecture like you want and achive best accuracy in 10 epochs.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
