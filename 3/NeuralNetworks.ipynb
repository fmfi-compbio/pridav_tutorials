{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will train some neural network using Pytorch.\n",
    "Installation instructions: https://pytorch.org/get-started/locally/\n",
    "\n",
    "I recommend to run this exercise in colab using GPU or in kaggle notebooks.\n",
    "You should find something like: Runtime -> Change runtime type -> T4 GPU to access GPU there.\n",
    "Everything thing you need is preinstalled there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because outside world is ugly (in console 'export OMP_NUM_THREADS=1')\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch is library for dealing with neural networks (and automatic gradients)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Torchvision is helper library for pytorch to deal with computer vision\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare dataset, whole lecture will be done over MNIST dataset\"\"\"\n",
    "batch_size_train = 256\n",
    "batch_size_test = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256]) tensor([3, 4, 5, 7, 7, 2, 8, 9, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# What is in train?\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape, y[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x73c95fb5d8d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X: 256 images, 1 channel (black/white), 28x28 image\n",
    "# We will flatten each image into one vector for now\n",
    "# Y: One number (category of image)\n",
    "# Images looks like this:\n",
    "\n",
    "plt.imshow(x[0,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight', torch.Size([10, 784])), ('bias', torch.Size([10]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple linear model (multiclass logistic regression)\n",
    "# This just computes scores for each class\n",
    "# This layer has parameters W, b and does (input.matmul(W.T) + b)\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "[(name, p.shape) for name, p in model_linear.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5201, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compute loss for one batch of images\n",
    "output = model_linear(x.flatten(1))\n",
    "log_probs = F.log_softmax(output, dim=-1)    # log_softmax(.) = log(softmax(.))\n",
    "loss = F.nll_loss(log_probs, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5201, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch has many loss functions, which look similar but take different things,\n",
    "# so take care\n",
    "# Here cross entropy(output, y) is same as nll_loss(log_softmax(output), y)\n",
    "loss2 = F.cross_entropy(output, y)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
       "        [ 0.0054,  0.0054,  0.0054,  ...,  0.0054,  0.0054,  0.0054],\n",
       "        [ 0.0156,  0.0156,  0.0156,  ...,  0.0156,  0.0156,  0.0156],\n",
       "        ...,\n",
       "        [ 0.0195,  0.0195,  0.0195,  ...,  0.0195,  0.0195,  0.0195],\n",
       "        [-0.0023, -0.0023, -0.0023,  ..., -0.0023, -0.0023, -0.0023],\n",
       "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to train model, we need to get gradients\n",
    "# This is easy, gradient will magically appear\n",
    "loss.backward()\n",
    "model_linear.weight.grad\n",
    "\n",
    "# If you really want to do things by hand, you can do\n",
    "# model_linear.weight.data = model_linear.weight.data - 0.01 * model_linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0078,  0.0078,  0.0078,  ...,  0.0078,  0.0078,  0.0078],\n",
       "        [ 0.0108,  0.0108,  0.0108,  ...,  0.0108,  0.0108,  0.0108],\n",
       "        [ 0.0312,  0.0312,  0.0312,  ...,  0.0312,  0.0312,  0.0312],\n",
       "        ...,\n",
       "        [ 0.0391,  0.0391,  0.0391,  ...,  0.0391,  0.0391,  0.0391],\n",
       "        [-0.0046, -0.0046, -0.0046,  ..., -0.0046, -0.0046, -0.0046],\n",
       "        [-0.0027, -0.0027, -0.0027,  ..., -0.0027, -0.0027, -0.0027]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Technical Pytorch note:\n",
    "# If we calculate gradient second time, it will accumulate, so we need to zero it out between steps (.zero_grad())\n",
    "output = model_linear(x.flatten(1))\n",
    "log_probs = F.log_softmax(output, dim=-1)    # log_softmax(.) = log(softmax(.))\n",
    "loss = F.nll_loss(log_probs, y)\n",
    "loss.backward()\n",
    "model_linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 2.0625456783365697\n",
      "epoch 1 training loss 1.215319841339233\n",
      "epoch 2 training loss 1.1928162664175033\n",
      "epoch 3 training loss 1.138574459070855\n",
      "epoch 4 training loss 1.1172642455456105\n"
     ]
    }
   ],
   "source": [
    "# Lets optimize, this is typical pytorch training loop you will see a lot with some modifications\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "optimizer = torch.optim.SGD(model_linear.parameters(), lr=1)\n",
    "\n",
    "# Epoch -> one pass through data\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    # Go through data, one chunk at a time\n",
    "    for x, y in train_loader:\n",
    "        # Here we calculate output from the model, note that we flatten the input (converts 256,1,28,28 to 256,784) \n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        # Here we calculate loss\n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        # Here we calculate gradients, first we need to zero previous ones\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        # And here we update the weights\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "    print(\"epoch\", epoch, \"training loss\", total_loss / total_loss_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 1.978 training accuracy 0.835 testing accuracy 0.889\n",
      "epoch 1 training loss 1.339 training accuracy 0.866 testing accuracy 0.840\n",
      "epoch 2 training loss 1.177 training accuracy 0.876 testing accuracy 0.875\n",
      "epoch 3 training loss 1.100 training accuracy 0.881 testing accuracy 0.856\n",
      "epoch 4 training loss 1.120 training accuracy 0.880 testing accuracy 0.905\n",
      "epoch 5 training loss 1.074 training accuracy 0.882 testing accuracy 0.897\n",
      "epoch 6 training loss 1.074 training accuracy 0.882 testing accuracy 0.880\n",
      "epoch 7 training loss 1.113 training accuracy 0.884 testing accuracy 0.870\n",
      "epoch 8 training loss 1.006 training accuracy 0.888 testing accuracy 0.879\n",
      "epoch 9 training loss 0.984 training accuracy 0.887 testing accuracy 0.848\n"
     ]
    }
   ],
   "source": [
    "# Optimize and compute training accuracy and test accuracy\n",
    "# Same loops as above, but we also calculate accuracy on training and testing dataset\n",
    "\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "optimizer = torch.optim.SGD(model_linear.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        # This calulcates accuracy\n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5141e-01, -2.0748e-01, -1.0756e-03, -2.1632e-01, -1.0040e-01,\n",
       "          3.0737e-01, -2.6616e-01, -2.8018e-01, -2.1607e-01, -2.8872e-02],\n",
       "        [ 2.7604e-01, -6.2164e-01, -2.4225e-01, -2.2411e-01,  1.0770e-01,\n",
       "         -1.0322e-01, -6.1034e-02, -1.9223e-01,  2.8147e-01,  8.7966e-02],\n",
       "        [ 1.2917e-01, -8.3329e-02, -5.8314e-02, -3.6307e-01, -1.7567e-01,\n",
       "          1.3651e-01,  8.4055e-03, -2.0000e-01, -2.6895e-01, -1.7928e-01],\n",
       "        [ 3.2809e-01, -2.9825e-01, -2.5020e-01,  1.1890e-01,  2.7453e-01,\n",
       "          1.2359e-01, -1.1562e-01, -1.7527e-01, -3.1563e-01, -1.4746e-01],\n",
       "        [ 2.8128e-01, -5.0026e-01, -2.6272e-01, -3.2388e-01,  1.7677e-01,\n",
       "          1.7635e-01,  9.7864e-02, -2.5453e-01, -3.9661e-02, -3.1104e-01],\n",
       "        [ 1.9265e-01, -2.3433e-01, -2.6200e-01, -5.5415e-02,  7.8274e-02,\n",
       "         -4.1907e-01,  1.7463e-01,  2.6926e-01, -6.1750e-02, -1.9966e-01],\n",
       "        [ 3.3291e-01, -4.9972e-01, -4.0462e-01, -6.2620e-02, -1.2973e-01,\n",
       "         -2.9446e-04, -4.1391e-01,  1.8809e-01, -2.1712e-01, -2.5540e-01],\n",
       "        [ 2.5846e-01, -4.0992e-01, -2.0775e-01,  1.4998e-01,  6.8670e-02,\n",
       "          1.6191e-02,  1.1658e-01, -1.4788e-01, -1.8098e-01, -1.5570e-01],\n",
       "        [ 8.4111e-02, -7.4086e-01, -3.8707e-01, -1.2157e-01, -5.5691e-02,\n",
       "          2.3480e-01,  1.1793e-01, -1.4922e-01, -1.4753e-01,  1.7653e-01],\n",
       "        [ 2.1344e-01,  1.5678e-01, -1.4624e-01, -4.7538e-02, -1.9458e-01,\n",
       "          2.6923e-01,  3.6227e-01, -2.0912e-01,  1.1389e-02, -3.3529e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how you define a simple neural network in Pytorch\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "model(torch.randn((10,28*28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 1.687 training accuracy 0.496 testing accuracy 0.495\n",
      "epoch 1 training loss 1.145 training accuracy 0.596 testing accuracy 0.652\n",
      "epoch 2 training loss 0.714 training accuracy 0.775 testing accuracy 0.758\n",
      "epoch 3 training loss 0.422 training accuracy 0.889 testing accuracy 0.779\n",
      "epoch 4 training loss 0.371 training accuracy 0.905 testing accuracy 0.866\n",
      "epoch 5 training loss 0.297 training accuracy 0.920 testing accuracy 0.839\n",
      "epoch 6 training loss 0.248 training accuracy 0.932 testing accuracy 0.939\n",
      "epoch 7 training loss 0.197 training accuracy 0.945 testing accuracy 0.945\n",
      "epoch 8 training loss 0.177 training accuracy 0.949 testing accuracy 0.906\n",
      "epoch 9 training loss 0.173 training accuracy 0.952 testing accuracy 0.932\n"
     ]
    }
   ],
   "source": [
    "# Let's train neural network, we only change model and maybe learning rate\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0.0)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = model(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        output = model(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# How to run on cuda (this optional, if you have GPU access, e.g. in colab or kaggle notebooks)\\n# No need to run this cell\\n\\nmodel = nn.Sequential(\\n    nn.Linear(28*28, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 10)\\n)\\n# Move model to GPU. Instead of x.cuda() you can also use x.to(device) where device is \"cpu\" or \"cuda\" so your code\\n# is nicely parametrized.\\n\\nmodel.cuda()\\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\\n\\nfor epoch in range(10):\\n    total_loss = 0\\n    total_loss_cc = 0\\n    total_good = 0\\n    total_samples = 0\\n    \\n    for x, y in train_loader:\\n        # Move input and labels to GPU\\n        output = model(x.cuda().flatten(1))\\n        y = y.cuda()\\n        log_probs = F.log_softmax(output, dim=-1)\\n        \\n        prediction = log_probs.argmax(dim=-1)\\n        total_good += (prediction == y).sum().item()\\n        total_samples += y.shape[0]\\n        \\n        batch_loss = F.nll_loss(log_probs, y)\\n        optimizer.zero_grad()\\n        batch_loss.backward()\\n        optimizer.step()\\n        total_loss += batch_loss.item()\\n        total_loss_cc += 1\\n        \\n    total_good_test = 0\\n    total_samples_test = 0\\n    for x, y in test_loader:\\n        # Move input and labels to GPU\\n        y = y.cuda()\\n        output = model(x.cuda().flatten(1))\\n        log_probs = F.log_softmax(output, dim=-1)\\n        \\n        prediction = log_probs.argmax(dim=-1)\\n        total_good_test += (prediction == y).sum().item()\\n        total_samples_test += y.shape[0]\\n        \\n    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \\n          \"training accuracy %.3f\" % (total_good / total_samples), \\n          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))\\n          \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# How to run on cuda (this optional, if you have GPU access, e.g. in colab or kaggle notebooks)\n",
    "# No need to run this cell\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "# Move model to GPU. Instead of x.cuda() you can also use x.to(device) where device is \"cpu\" or \"cuda\" so your code\n",
    "# is nicely parametrized.\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        # Move input and labels to GPU\n",
    "        output = model(x.cuda().flatten(1))\n",
    "        y = y.cuda()\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        # Move input and labels to GPU\n",
    "        y = y.cuda()\n",
    "        output = model(x.cuda().flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))\n",
    "          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 1.\\nIn example above figure out best settings for learning rate a momentum in the optimizer.\\nTypical values for momentum are something like 0.8, 0.9, 0.95, 0.99.\\nKeep the number of epochs to 10. Also keep the learning constant during training.\\nSumarize your findings in some chart or table.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 1.\n",
    "In example above figure out best settings for learning rate a momentum in the optimizer.\n",
    "Typical values for momentum are something like 0.8, 0.9, 0.95, 0.99.\n",
    "Keep the number of epochs to 10. Also keep the learning constant during training.\n",
    "Sumarize your findings in some chart or table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 2.\\nHere is network with 3 hidden layers (each additional hidden layer we add nn.Linear(256,256) and nn.ReLU())\\n\\nmodel = nn.Sequential(\\n    nn.Linear(28*28, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 256),\\n    nn.ReLU(),\\n    nn.Linear(256, 10)\\n)\\n\\nHow does adding more hidden layers influence final accuracy (try 3-6 hidden layers).\\nAlways figure out the best optimizer settings. Again run for 10 epochs.\\nAgain produce some chart or table (maybe with accuracy changes over time).\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 2.\n",
    "Here is network with 3 hidden layers (each additional hidden layer we add nn.Linear(256,256) and nn.ReLU())\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "How does adding more hidden layers influence final accuracy (try 3-6 hidden layers).\n",
    "Always figure out the best optimizer settings. Again run for 10 epochs.\n",
    "Again produce some chart or table (maybe with accuracy changes over time).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere is an example of a simple convolutional network. They are really good on images.\\nE.g. ResNet is a very strong baseline for any image task https://pytorch.org/vision/main/models/resnet.html\\nOr convnext: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/convnext.py\\n\\nYou do not need this for the exercise it is just a reference.\\n\\nmodel = nn.Sequential(\\n    nn.Conv2d(1, 16, 5, padding=2, stride=2),  # Here we get 14x14 image with 16 channels\\n    nn.ReLU(),\\n    nn.Conv2d(16, 16, 3, padding='same'),\\n    nn.ReLU(),\\n    nn.Conv2d(16, 32, 3, padding=1, stride=2),  # Here we get 7x7 image with 32 channels\\n    nn.ReLU(),\\n    nn.Conv2d(32, 32, 3, padding='same'),\\n    nn.ReLU(),\\n    nn.Flatten(),\\n    nn.Linear(7*7*32, 10)\\n)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here is an example of a simple convolutional network. They are really good on images.\n",
    "E.g. ResNet is a very strong baseline for any image task https://pytorch.org/vision/main/models/resnet.html\n",
    "Or convnext: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/convnext.py\n",
    "\n",
    "You do not need this for the exercise it is just a reference.\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 5, padding=2, stride=2),  # Here we get 14x14 image with 16 channels\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, padding=1, stride=2),  # Here we get 7x7 image with 32 channels\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(7*7*32, 10)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[-1.2736e-01, -5.0541e-02,  1.6419e-02, -3.8433e-02, -7.8077e-02,\n",
      "         -9.6364e-02, -1.8109e-01,  3.7158e-02, -1.0903e-01, -1.2582e-01,\n",
      "         -1.9172e-02, -2.3187e-01, -1.6782e-01, -1.6346e-01, -1.1458e-01,\n",
      "         -6.6168e-02, -1.5588e-01, -1.6817e-01, -8.5721e-03, -2.3697e-01,\n",
      "         -2.3173e-02, -2.5765e-01,  8.2985e-02, -1.6923e-02, -1.3278e-01,\n",
      "         -1.9925e-01, -3.8409e-02, -5.1828e-02, -1.5713e-01, -1.4066e-01,\n",
      "          4.6459e-02, -6.1371e-02, -1.7689e-01, -1.3830e-01, -2.2531e-02,\n",
      "         -1.3340e-02, -1.4723e-02,  7.8735e-02, -2.3118e-01,  7.2562e-02,\n",
      "         -1.0030e-01,  8.5526e-02, -3.0897e-02, -8.1466e-02, -3.5811e-02,\n",
      "          4.9752e-02, -1.3962e-01,  7.3934e-02, -1.0844e-01, -1.6311e-02,\n",
      "         -1.2112e-01, -7.4963e-02,  1.4158e-02, -3.0011e-01, -1.3850e-01,\n",
      "         -4.7454e-02, -6.5603e-02, -1.4805e-01, -7.6835e-02, -4.2714e-02,\n",
      "         -2.8854e-02, -1.2255e-01, -4.5716e-02, -1.0402e-01, -2.0806e-01,\n",
      "         -1.2824e-01, -1.2270e-01, -1.7784e-01, -1.8510e-01,  1.2079e-01,\n",
      "         -4.5091e-02, -5.9626e-02, -9.7595e-02, -1.1799e-01, -7.0208e-02,\n",
      "         -9.9531e-02, -9.1888e-02, -2.0764e-01,  1.7561e-01, -1.9484e-01,\n",
      "          9.1382e-02, -5.5827e-02, -1.4848e-01,  7.2079e-02, -1.3875e-01,\n",
      "         -1.7494e-01,  1.3289e-01, -1.8194e-01, -1.2337e-01, -7.9483e-02,\n",
      "         -1.9039e-01, -1.4699e-01, -2.5803e-01, -2.4343e-01, -2.5212e-02,\n",
      "         -1.3428e-01, -1.9547e-01, -1.3983e-01, -2.9197e-01, -3.6837e-01,\n",
      "         -3.1397e-02, -1.5277e-01,  2.3798e-02, -1.5790e-01, -9.9294e-02,\n",
      "         -1.3566e-01, -4.3370e-02,  1.2911e-01, -2.4190e-01, -3.1691e-02,\n",
      "         -1.2826e-01, -7.3786e-02, -4.3709e-02, -1.7672e-01, -1.5473e-02,\n",
      "         -2.1899e-01, -1.2247e-01, -1.4798e-01, -2.3140e-01, -5.2487e-02,\n",
      "         -2.8872e-01, -1.2819e-01, -1.6690e-01, -5.6316e-01, -5.3063e-01,\n",
      "         -5.9273e-01, -4.7836e-01, -4.0084e-01, -4.8900e-01, -9.3846e-02,\n",
      "         -3.6156e-02,  8.3634e-02,  5.7640e-03,  1.9549e-02, -2.4568e-01,\n",
      "         -5.1410e-02, -1.8885e-01, -2.7211e-03,  1.3235e-02, -2.4504e-01,\n",
      "          4.2115e-02, -1.6689e-01, -1.7724e-01, -2.1750e-01,  4.4064e-02,\n",
      "         -1.7879e-01, -1.2308e-01, -2.5702e-01, -1.6400e-01, -4.6757e-01,\n",
      "         -9.1526e-02, -4.3434e-01, -3.9673e-01, -5.3385e-01, -8.4026e-01,\n",
      "         -8.4138e-01, -7.4429e-01, -3.3133e-01, -3.7082e-01, -1.8352e-01,\n",
      "         -9.3052e-03,  1.5511e-01, -1.2162e-01, -3.3234e-01, -7.0413e-02,\n",
      "         -3.2322e-01,  4.2741e-02, -1.6522e-01, -1.6882e-01, -1.2394e-01,\n",
      "         -1.2009e-01, -5.8890e-02, -1.4487e-01, -2.5187e-01, -1.7647e-01,\n",
      "         -1.1923e-01, -3.7628e-01, -4.4125e-01, -1.3846e-01, -3.1576e-01,\n",
      "         -3.8842e-01, -6.6237e-01, -5.3600e-01, -5.2062e-01, -4.3192e-01,\n",
      "         -4.7093e-01, -5.9441e-01, -3.0104e-01,  3.1776e-02,  2.1597e-01,\n",
      "          2.6984e-03,  7.8337e-02, -1.2972e-01, -7.3484e-02, -4.7584e-02,\n",
      "         -1.6180e-01, -9.9679e-02, -2.9170e-02, -6.6808e-02, -4.0408e-03,\n",
      "          1.1796e-02, -1.1512e-01, -1.1080e-01, -2.4152e-01, -6.2975e-01,\n",
      "         -4.7448e-01, -1.8339e-01, -2.7741e-01, -7.0447e-01, -6.5165e-01,\n",
      "         -2.7585e-01, -5.4974e-01, -5.2465e-01, -4.3145e-01, -3.1980e-01,\n",
      "         -3.2432e-01,  1.4921e-01,  1.2514e-01,  3.3546e-01,  4.2509e-01,\n",
      "          2.5890e-01, -9.8217e-02, -1.0775e-01, -1.4741e-01,  1.0219e-02,\n",
      "         -1.2855e-01, -1.2121e-01,  5.5995e-02,  2.0656e-02,  3.6215e-02,\n",
      "         -2.2770e-01, -1.5537e-01, -3.1128e-01, -1.4368e-01, -2.6511e-01,\n",
      "         -5.4844e-01, -6.9630e-01, -3.9641e-01, -2.1832e-01, -1.3728e-01,\n",
      "         -4.3134e-01, -2.9657e-01, -1.8564e-01, -1.2479e-01,  1.4771e-01,\n",
      "          1.6801e-01,  3.4287e-01,  4.5213e-01,  2.9541e-01, -1.0026e-01,\n",
      "         -2.5885e-02, -9.9447e-02,  1.3730e-02, -1.4382e-01, -2.2051e-02,\n",
      "         -9.3162e-02,  2.1003e-02, -1.6147e-01, -1.8410e-01, -1.9329e-01,\n",
      "         -4.0857e-01, -7.4256e-02, -1.9926e-01, -5.4617e-01, -7.0183e-01,\n",
      "         -1.2866e-01,  4.1310e-01,  1.0741e-01,  4.9012e-02, -9.9600e-02,\n",
      "          1.1888e-01,  1.1956e-01, -1.2503e-01, -1.0791e-01, -1.1980e-01,\n",
      "          1.2578e-01,  3.5590e-01,  2.0579e-01, -1.3202e-01,  3.0930e-02,\n",
      "         -7.9699e-02,  1.1116e-01,  1.2329e-02, -1.0183e-01, -3.8311e-02,\n",
      "         -3.2383e-02, -6.0291e-02,  2.2394e-02,  9.9006e-02,  1.3817e-01,\n",
      "          1.4054e-01, -5.5979e-02, -2.2731e-01,  8.4183e-03,  3.6813e-01,\n",
      "          2.2851e-01,  1.2108e-01,  1.1587e-01, -1.3546e-01, -1.7342e-01,\n",
      "         -1.6193e-01, -2.5067e-01, -1.6084e-03,  1.8143e-01,  2.8879e-01,\n",
      "          1.6461e-01,  9.0014e-03, -1.2233e-01, -1.5267e-01,  6.8687e-02,\n",
      "         -1.3491e-01, -1.3445e-01, -7.9186e-03, -9.6011e-02, -3.6013e-02,\n",
      "          2.7770e-01,  2.4308e-01,  2.1979e-01,  1.0878e-01,  8.2318e-02,\n",
      "          6.1272e-03,  2.8191e-01,  1.7851e-01, -1.9864e-01,  1.8119e-01,\n",
      "         -3.4602e-01, -6.4079e-01, -6.6995e-01, -7.1572e-01, -5.2197e-01,\n",
      "         -1.2599e-01,  2.1346e-01,  2.4509e-01, -1.9927e-01,  5.0778e-02,\n",
      "          5.0672e-02, -1.3638e-01, -1.6642e-01, -1.3306e-01, -3.2654e-02,\n",
      "         -2.5849e-02, -1.5239e-01,  1.8266e-01,  3.4051e-01,  6.2670e-01,\n",
      "          3.1612e-01,  2.7479e-01, -2.0390e-01, -8.6850e-02,  1.4034e-01,\n",
      "          5.1961e-02, -6.6356e-02, -1.2750e-01, -5.5063e-01, -1.0202e+00,\n",
      "         -9.8250e-01, -7.9346e-01, -4.7116e-01, -3.9919e-01,  1.2116e-01,\n",
      "          1.7330e-01,  1.9557e-02, -5.4711e-02, -3.7660e-02, -1.7776e-01,\n",
      "         -6.1836e-02, -1.6894e-01, -1.0693e-01,  1.0843e-01, -4.0826e-02,\n",
      "          2.0817e-01,  4.0235e-01,  7.4697e-01,  6.3473e-01,  2.8490e-01,\n",
      "          3.0465e-02, -3.8425e-01, -1.8284e-02, -2.0920e-01,  4.9554e-02,\n",
      "         -1.4469e-02, -6.2530e-01, -5.3942e-01, -8.2500e-01, -6.2367e-01,\n",
      "         -6.2524e-01, -2.8983e-01, -1.0392e-02,  1.2416e-02,  4.4158e-02,\n",
      "         -3.2666e-02, -7.3175e-02, -4.3968e-02, -1.5611e-01,  2.8552e-02,\n",
      "         -4.6837e-02, -1.8643e-01,  6.1352e-02,  3.7159e-01,  5.5251e-01,\n",
      "          7.6452e-01,  6.8248e-01,  2.0320e-01,  3.4534e-01,  2.5032e-01,\n",
      "          3.8471e-01,  3.3371e-01,  9.4083e-01,  3.7531e-01, -3.7374e-02,\n",
      "         -1.6486e-01, -2.8122e-01, -4.4011e-01, -3.2053e-01, -1.0978e-01,\n",
      "         -1.2917e-01, -1.1948e-01, -7.7801e-02, -1.4023e-01, -7.3544e-02,\n",
      "         -5.7086e-02,  2.0312e-03, -5.7331e-02, -1.2839e-01, -5.3735e-02,\n",
      "          2.7277e-01,  5.5881e-01,  5.8335e-01,  7.0359e-01,  4.5363e-01,\n",
      "          4.2434e-01,  3.4902e-01,  1.2463e-01,  4.5942e-01,  6.9995e-01,\n",
      "          9.6688e-01,  4.6838e-01,  1.9095e-02, -3.9117e-02, -2.0262e-02,\n",
      "         -1.3656e-01,  6.9856e-03, -1.0034e-02, -5.5790e-02,  5.8591e-02,\n",
      "         -2.2804e-01, -2.5227e-02, -1.1948e-01, -1.9210e-01,  1.3822e-01,\n",
      "         -2.7638e-02, -2.1178e-01, -2.2345e-01,  1.6386e-01,  2.0926e-01,\n",
      "          2.2126e-01,  4.5472e-02, -8.2682e-02, -7.9613e-02, -6.8939e-02,\n",
      "         -2.4360e-01,  2.0438e-01,  7.0713e-01,  8.3157e-01,  3.3533e-01,\n",
      "         -5.5614e-02,  1.4866e-01, -7.2497e-02, -3.0315e-01,  1.5495e-01,\n",
      "         -9.3961e-03, -2.3440e-02, -1.8905e-01, -2.4429e-02,  1.5747e-01,\n",
      "          4.4436e-02, -2.2081e-01, -9.8436e-02, -2.3199e-01, -2.6954e-01,\n",
      "         -2.4737e-01,  1.3653e-01,  9.4316e-02,  1.4754e-01, -4.3978e-01,\n",
      "         -5.0986e-01, -3.2998e-01, -7.9868e-01, -5.4622e-01,  1.5367e-01,\n",
      "          8.3806e-02,  4.0125e-01,  2.2697e-01, -8.3166e-02, -3.6117e-01,\n",
      "         -4.3129e-01, -3.1676e-01, -2.6522e-02,  7.6512e-02, -1.4073e-02,\n",
      "         -7.2195e-02, -1.6360e-01, -1.7052e-01, -9.9307e-03, -3.2913e-02,\n",
      "         -2.3900e-01, -3.6224e-02, -1.2891e-01, -2.8538e-01, -2.7597e-01,\n",
      "          2.7646e-01, -9.8955e-03, -5.5128e-01, -8.3709e-01, -1.0220e+00,\n",
      "         -1.1420e+00, -4.5198e-01, -1.1022e-01,  1.9229e-01,  2.2371e-02,\n",
      "         -1.4129e-01, -3.5257e-01, -4.9505e-01, -7.6672e-01, -7.1836e-01,\n",
      "         -7.7990e-02, -9.3235e-02, -1.6113e-01, -1.4917e-01, -3.9206e-02,\n",
      "         -8.5721e-02, -2.6890e-02, -4.7867e-03,  3.5962e-02, -1.8759e-02,\n",
      "          9.1920e-02, -2.7385e-01, -1.6937e-01, -2.9728e-01, -2.6635e-01,\n",
      "         -8.3834e-01, -1.2709e+00, -1.2322e+00, -7.7800e-01, -1.0051e-01,\n",
      "          1.6559e-01,  1.5922e-01, -1.3962e-01, -3.6224e-01, -3.4379e-01,\n",
      "         -5.7133e-01, -5.8819e-01, -5.9886e-01, -4.7201e-01, -9.9732e-02,\n",
      "         -1.2544e-01, -1.8621e-01, -1.7594e-01, -1.1362e-01, -1.0386e-01,\n",
      "         -1.1349e-02, -1.4018e-01,  2.5527e-02, -4.7743e-02, -1.2798e-01,\n",
      "         -1.9976e-03, -1.7518e-01, -5.7885e-01, -8.2905e-01, -1.0959e+00,\n",
      "         -1.0979e+00, -4.5693e-01, -2.9118e-01, -3.3110e-01, -1.1310e-01,\n",
      "         -1.5085e-01, -3.4001e-01, -1.9115e-01, -4.1034e-01, -6.2014e-01,\n",
      "         -5.1896e-01, -2.6840e-01, -2.7447e-01, -1.6731e-01, -9.7285e-02,\n",
      "         -9.1988e-02, -2.0040e-01, -1.2092e-01, -1.5742e-01,  8.3836e-05,\n",
      "         -1.1287e-01, -6.3355e-02, -2.5050e-01, -4.1726e-02, -1.7525e-01,\n",
      "         -7.1812e-01, -6.0345e-01, -7.9593e-01, -7.6602e-01, -4.8283e-01,\n",
      "         -4.3333e-01, -4.6617e-01, -4.9511e-01, -2.5916e-01, -3.1951e-01,\n",
      "         -2.1195e-01, -2.4909e-01, -4.2245e-01, -5.1363e-01, -3.3196e-01,\n",
      "         -2.6191e-01, -8.1436e-02, -2.4237e-01, -2.2617e-01, -4.1117e-02,\n",
      "         -1.3529e-01, -7.4477e-02, -2.3263e-01,  3.8354e-02, -4.8471e-02,\n",
      "         -1.1686e-01, -2.6410e-01, -3.8910e-01, -6.0953e-01, -3.8376e-01,\n",
      "         -4.3529e-01, -6.0366e-01, -5.6798e-01, -3.1299e-01, -2.5841e-01,\n",
      "         -1.1811e-01, -1.5281e-01, -1.2846e-01,  2.8625e-02,  8.4451e-02,\n",
      "          3.4299e-04, -1.9457e-01, -2.4118e-01, -3.3484e-02, -7.2301e-02,\n",
      "         -6.0535e-02,  2.5929e-02, -1.3675e-01, -9.3451e-02, -1.1935e-01,\n",
      "         -6.9345e-03, -7.4440e-02, -8.7392e-02, -5.0731e-02, -2.2017e-01,\n",
      "         -1.9429e-01, -3.6740e-01, -1.9597e-01, -2.1023e-01,  2.3839e-02,\n",
      "         -5.9821e-02,  3.0284e-02,  1.4505e-01,  3.6078e-01,  2.4499e-01,\n",
      "          7.6664e-02, -3.5091e-02,  4.6180e-02, -5.9871e-02, -3.6138e-02,\n",
      "         -1.2109e-01, -6.1468e-03, -1.9703e-01, -1.9478e-01, -1.0276e-01,\n",
      "         -2.0628e-01, -6.5772e-02, -4.6595e-02, -1.8050e-01, -6.6598e-02,\n",
      "         -1.3713e-01, -6.2063e-02, -4.6739e-02, -1.5021e-01, -1.3351e-01,\n",
      "         -1.0561e-01, -3.0098e-02, -1.2758e-01, -2.2970e-01,  6.0697e-02,\n",
      "          2.1274e-01,  3.3710e-01,  1.8792e-01, -8.0033e-02, -9.7464e-02,\n",
      "         -1.6101e-01, -2.2862e-01, -1.5395e-01, -2.8272e-01, -1.1099e-01,\n",
      "         -2.6732e-02, -2.0165e-01, -1.9984e-02,  9.7744e-02, -2.5762e-01,\n",
      "         -1.5676e-01, -2.6519e-02, -1.5022e-01, -2.2877e-01,  4.3991e-02,\n",
      "         -8.1334e-02,  1.4705e-02,  6.1594e-02, -4.7740e-03, -1.8359e-01,\n",
      "          2.3664e-02, -6.6396e-02,  3.0416e-02, -1.4156e-01, -2.1374e-01,\n",
      "         -8.5957e-02, -1.2756e-01, -1.5869e-01, -1.2024e-01,  3.6291e-03,\n",
      "         -2.0216e-01, -2.0228e-02, -1.1198e-01, -1.7087e-01, -1.2851e-01,\n",
      "         -1.2763e-01, -1.7588e-01, -1.3561e-01,  5.6067e-03, -8.2210e-02,\n",
      "         -6.1678e-02,  5.6532e-02, -5.7454e-02,  1.0196e-01, -5.7153e-02,\n",
      "         -3.0271e-02, -1.4909e-01, -4.8938e-02, -1.9684e-01, -7.8718e-02,\n",
      "          5.3724e-02, -2.5302e-02, -5.8690e-02, -2.5312e-01, -8.1666e-02,\n",
      "         -2.6004e-01, -8.3177e-03, -1.8289e-01, -1.8410e-02, -1.9009e-01,\n",
      "         -8.6740e-02, -1.5377e-01, -3.8671e-02, -9.8699e-02, -5.6059e-02,\n",
      "          1.9046e-02, -1.6596e-01, -1.5523e-01,  3.4588e-03, -5.1250e-02,\n",
      "         -4.9197e-02, -1.1678e-01,  6.5658e-03, -4.3613e-02, -1.4189e-01,\n",
      "          4.7230e-02,  3.6583e-02, -7.4055e-02, -1.3091e-02, -1.7347e-01,\n",
      "         -9.5834e-02,  5.3221e-02, -9.8715e-02,  1.0687e-01, -1.8454e-01,\n",
      "         -1.8282e-02, -7.0011e-02, -2.2091e-02, -6.5518e-02, -4.8427e-02,\n",
      "         -2.2463e-02, -9.6503e-02, -2.5954e-01, -4.6736e-02]])\n"
     ]
    }
   ],
   "source": [
    "# One nice things about neural networks is that you can also get a gradient of the output with respect to the input.\n",
    "# But as usual, there is a small catch\n",
    "x = torch.randn(1,28*28)  # This is just a placeholder input\n",
    "model(x)[0,0].backward()  # We want gradient of output [0,0]\n",
    "print(x.grad) # But there is not gradient\n",
    "\n",
    "# We need to tell Pytorch, that we want that gradient\n",
    "x = torch.randn(1,28*28)  # This is just a placeholder input\n",
    "x.requires_grad_(True)    # This is the trick\n",
    "model(x)[0,0].backward()  # We want gradient of output [0,0]\n",
    "print(x.grad) # Now we have a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTask 3:\\nChanging one to seven in boring.\\nBut let's change eight to one.\\n\\nDo following:\\n1) Take some reasonably trained neural network from previous tasks (or train a new one)\\n2) Pick an image from test set, which has label 8 and neural network also predicts 8\\n3) Make as small change as possible, so that the prediction on that image is 1 \\n\\nHint 1: Use that gradient of input with respect to output\\nHint 2: Maybe you need multiple steps\\nHint 3: Anything reasonable is good here, do not obsess about some global optimum\\n\\n4) Visualize both images (original 8 and changed one, which gets prediction 1).\\n5) Visualize the difference (maybe you need to amplify it)\\n6) How do you feel about the result?\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 3:\n",
    "Changing one to seven in boring.\n",
    "But let's change eight to one.\n",
    "\n",
    "Do following:\n",
    "1) Take some reasonably trained neural network from previous tasks (or train a new one)\n",
    "2) Pick an image from test set, which has label 8 and neural network also predicts 8\n",
    "3) Make as small change as possible, so that the prediction on that image is 1 \n",
    "\n",
    "Hint 1: Use that gradient of input with respect to output\n",
    "Hint 2: Maybe you need multiple steps\n",
    "Hint 3: Anything reasonable is good here, do not obsess about some global optimum\n",
    "\n",
    "4) Visualize both images (original 8 and changed one, which gets prediction 1).\n",
    "5) Visualize the difference (maybe you need to amplify it)\n",
    "6) How do you feel about the result?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
