{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this exercise we will train some neural network using Pytorch.\n",
    "Installation instructions: https://pytorch.org/get-started/locally/\n",
    "\n",
    "I recommend to run this exercise in colab using GPU or in kaggle notebooks.\n",
    "You should find something like: Runtime -> Change runtime type -> T4 GPU to access GPU there.\n",
    "Everything thing you need is preinstalled there.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because outside world is ugly\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch is library for dealing with neural networks (and automatic gradients)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Torchvision is helper library for pytorch to deal with computer vision\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare dataset, whole lecture will be done over MNIST dataset\"\"\"\n",
    "batch_size_train = 256\n",
    "batch_size_test = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is in train?\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape, y[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 256 images, 1 channel (black/white), 28x28 image\n",
    "# We will flatten each image into one vector for now\n",
    "# Y: One number (category of image)\n",
    "# Images looks like this:\n",
    "\n",
    "plt.imshow(x[0,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear model from last lecture\n",
    "# This just computes scores for each class\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "[(name, p.shape) for name, p in model_linear.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute loss for one batch of images\n",
    "output = model_linear(x.flatten(1))\n",
    "log_probs = F.log_softmax(output, dim=-1)    # log_softmax(.) = log(softmax(.))\n",
    "loss = F.nll_loss(log_probs, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch has many loss functions, which look similar but take different things,\n",
    "# so take care\n",
    "# Here cross entropy(output, y) is same as nll_loss(log_softmax(output), y)\n",
    "loss2 = F.cross_entropy(output, y)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to train model, we need to get gradients\n",
    "# This is easy, gradient will magically appear\n",
    "loss.backward()\n",
    "model_linear.weight.grad\n",
    "\n",
    "# If you really want to do things by hand\n",
    "# model_linear.weight.data = model_linear.weight.data - 0.01 * model_linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we calculate gradient second time, it will accumulate\n",
    "output = model_linear(x.flatten(1))\n",
    "log_probs = F.log_softmax(output, dim=-1)    # log_softmax(.) = log(softmax(.))\n",
    "loss = F.nll_loss(log_probs, y)\n",
    "loss.backward()\n",
    "model_linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets optimize, this is typical pytorch training loop you will see a lot with some modifications\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "optimizer = torch.optim.SGD(model_linear.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    for x, y in train_loader:\n",
    "        # Here we calculate output from the model, note that we flatten the input (converts 256,1,28,28 to 256,784) \n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        # Here we calculate loss\n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        # Here we calculate gradients, first we need to zero previous ones\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        # And here we update the weights\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "    print(\"epoch\", epoch, \"training loss\", total_loss / total_loss_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize and compute training accuracy and test accuracy\n",
    "\n",
    "model_linear = nn.Linear(28*28, 10)\n",
    "optimizer = torch.optim.SGD(model_linear.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        # This calulcates accuracy\n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        output = model_linear(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train neural network, we only change model and maybe learning rate\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0.0)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = model(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        output = model(x.flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# How to run on cuda (this optional, if you have GPU access, vyuka does not have one, colab does)\n",
    "# No need to run this cell\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "# Move model to GPU. Instead of x.cuda() you can also use x.to(device) where device is \"cpu\" or \"cuda\" so your code\n",
    "# is nicely parametrized.\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_loss_cc = 0\n",
    "    total_good = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        # Move input and labels to GPU\n",
    "        output = model(x.cuda().flatten(1))\n",
    "        y = y.cuda()\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good += (prediction == y).sum().item()\n",
    "        total_samples += y.shape[0]\n",
    "        \n",
    "        batch_loss = F.nll_loss(log_probs, y)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "        total_loss_cc += 1\n",
    "        \n",
    "    total_good_test = 0\n",
    "    total_samples_test = 0\n",
    "    for x, y in test_loader:\n",
    "        # Move input and labels to GPU\n",
    "        y = y.cuda()\n",
    "        output = model(x.cuda().flatten(1))\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        prediction = log_probs.argmax(dim=-1)\n",
    "        total_good_test += (prediction == y).sum().item()\n",
    "        total_samples_test += y.shape[0]\n",
    "        \n",
    "    print(\"epoch\", epoch, \"training loss %.3f\" % (total_loss / total_loss_cc), \n",
    "          \"training accuracy %.3f\" % (total_good / total_samples), \n",
    "          \"testing accuracy %.3f\" % (total_good_test / total_samples_test))\n",
    "          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 1.\n",
    "In example above figure out best settings for learning rate a momentum in the optimizer.\n",
    "Keep the number of epochs to 10. Also keep the learning constant during training.\n",
    "Sumarize your findings in some chart or table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 2.\n",
    "Here is network with 2 hidden layers (each additional hidden layer we add nn.Linear(256,256) and nn.ReLU())\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "How does adding more hidden layers influence final accuracy (try 3-6 hidden layers).\n",
    "Always figure out the best optimizer settings. Again run for 10 epochs.\n",
    "Again produce some chart or table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 3.\n",
    "Here is an example of convolutional network. Do not change it for this task.\n",
    "Only figure out best settings for the optimizer. Also run for only 10 epochs.\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 5, padding=1, stride=2),  # Here we get 14x14 image with 16 channels\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, padding=1, stride=2),  # Here we get 7x7 image with 32 channels\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(7*7*32, 10)\n",
    ")\n",
    "\n",
    "Run training of convolutional network and report the results and compare with best results from above.\n",
    "Also, there is no need to flatten the input now. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 4 bonus:\n",
    "Change neural architecture like you want and achive best accuracy in 10 epochs.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
